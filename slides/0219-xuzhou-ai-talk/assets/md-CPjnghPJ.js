import{o as i,b as n,w as o,g as e,ad as t,v as r,x as c,T as a}from"./modules/vue-DdBzI6iR.js";import{I as x}from"./slidev/default-DMjDit6e.js";import{u as m,f as g}from"./slidev/context-QOEjJLWk.js";import"./index-By41BRkw.js";import"./modules/shiki-DKM40GFB.js";const S={__name:"slides.md__slidev_28",setup(p){const{$clicksContext:l,$frontmatter:d}=m();return l.setup(),(b,s)=>(i(),n(x,r(c(a(g)(a(d),27))),{default:o(()=>[...s[0]||(s[0]=[e("h1",null,"Scaling Law：大力出奇迹",-1),e("h3",null,"规模增长带来可预测的性能提升——大模型时代最核心的经验法则",-1),e("div",{class:"grid grid-cols-5 gap-4 mt-3 max-w-4xl mx-auto"},[e("div",{class:"col-span-3 space-y-3"},[e("div",{class:"rounded-xl border border-indigo-200 bg-white shadow-sm p-4"},[e("div",{class:"flex items-center gap-2 mb-2"},[e("div",{class:"rounded-lg bg-indigo-500 text-white px-2.5 py-1 font-bold text-sm"},"OpenAI 2020"),e("span",{class:"text-xs text-slate-500"},[t("Kaplan et al., "),e("i",null,"Scaling Laws for Neural Language Models")])]),e("div",{class:"text-center py-3 bg-slate-50 rounded-lg"},[e("div",{class:"font-mono text-lg text-indigo-700"},[t("L(x) ∝ x"),e("sup",null,"−α")]),e("div",{class:"text-xs text-slate-500 mt-1"},[t("x = N（参数量）/ D（数据量）/ C（计算量），损失随规模"),e("b",null,"幂律下降")])]),e("div",{class:"mt-3 grid grid-cols-3 gap-2 text-center text-xs"},[e("div",{class:"rounded bg-indigo-50 p-2"},[e("div",{class:"text-indigo-600 font-bold text-sm"},"N 参数量"),e("div",{class:"text-slate-400"},"α ≈ 0.076")]),e("div",{class:"rounded bg-indigo-50 p-2"},[e("div",{class:"text-indigo-600 font-bold text-sm"},"D 数据量"),e("div",{class:"text-slate-400"},"α ≈ 0.095")]),e("div",{class:"rounded bg-indigo-50 p-2"},[e("div",{class:"text-indigo-600 font-bold text-sm"},"C 计算量"),e("div",{class:"text-slate-400"},"α ≈ 0.050")])])]),e("div",{class:"rounded-xl border border-amber-200 bg-white shadow-sm p-3"},[e("div",{class:"flex items-center gap-2 mb-1"},[e("div",{class:"rounded-lg bg-amber-500 text-white px-2.5 py-1 font-bold text-xs"},"Chinchilla 2022"),e("span",{class:"text-xs text-slate-500"},"Hoffmann et al., DeepMind")]),e("div",{class:"text-xs text-slate-600"},[t(" 修正 Kaplan 结论：给定算力预算，参数和数据应"),e("span",{class:"text-amber-600 font-semibold"},"等比例扩展"),t("（N ∝ D）。 GPT-3（175B）其实数据量不够——同等算力下，"),e("span",{class:"text-amber-700 font-medium"},"更小模型 + 更多数据"),t("效果更好。 ")])])]),e("div",{class:"col-span-2 space-y-3"},[e("div",{class:"rounded-xl border border-rose-200 bg-white shadow-sm p-3"},[e("div",{class:"text-rose-600 font-bold text-sm mb-1"},"「涌现」：真的还是幻觉？"),e("div",{class:"text-xs text-slate-600 space-y-1"},[e("div",null,[e("b",null,"2022"),t(" Wei et al.：某些能力在规模突破阈值时"),e("span",{class:"text-rose-500 font-medium"},"突然出现"),t("——被称为「涌现能力」")]),e("div",{class:"rounded bg-rose-50 px-2 py-1 text-rose-700"},[e("b",null,"2023"),t(" Schaeffer et al. 反驳：涌现是"),e("b",null,"评估指标的假象"),t("——换用连续指标后，能力随规模平滑增长，并无突变")]),e("div",{class:"text-slate-500"},"启示：Scaling Law 本身是平滑的，「突变」更多源于离散指标的测量偏差")])]),e("div",{class:"rounded-xl border-2 border-emerald-300 bg-emerald-50/50 shadow-sm p-3"},[e("div",{class:"text-emerald-700 font-bold text-sm mb-1"},"Training Scaling → Test-time Scaling"),e("div",{class:"text-xs text-slate-600 space-y-1"},[e("div",null,[e("span",{class:"font-medium"},"Training Scaling"),t("：更大模型、更多数据、更长训练")]),e("div",{class:"text-slate-500"},"→ 算力成本指数增长，收益递减（撞墙）"),e("div",{class:"border-t border-dashed border-slate-300 my-1"}),e("div",null,[e("span",{class:"font-medium text-emerald-600"},"Test-time Scaling"),t("：推理时让模型多「想一会儿」")]),e("div",{class:"text-slate-500"},"→ 增加推理计算 = 思考更深、搜索更广"),e("div",{class:"rounded bg-emerald-100 px-2 py-1 text-emerald-700 font-medium mt-1"},"把算力从训练阶段部分转移到推理阶段，打开全新的 Scaling 维度")])])])],-1),e("div",{class:"mt-3 text-center text-xs text-slate-500"},"Scaling Law 不仅指导了训练范式，也催生了 Reasoning 模型——当 Training Scaling 趋近天花板，Test-time Scaling 成为新前沿",-1)])]),_:1},16))}};export{S as default};
