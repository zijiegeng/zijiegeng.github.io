import{o,b as d,w as r,g as t,ad as s,v as m,x as i,T as e}from"./modules/vue-B4QAKlIR.js";import{I as x}from"./slidev/default-CVwbI38Z.js";import{u as c,f as p}from"./slidev/context-B-aIOMhn.js";import"./index-DrKhBKue.js";import"./modules/shiki-DVNyJe3i.js";const h={__name:"slides.md__slidev_11",setup(b){const{$clicksContext:l,$frontmatter:n}=c();return l.setup(),(u,a)=>(o(),d(x,m(i(e(p)(e(n),10))),{default:r(()=>[...a[0]||(a[0]=[t("h1",null,"行为主义：在环境中通过奖励塑造行为",-1),t("h3",null,"核心思想：智能 = 与环境交互，最大化长期奖励",-1),t("div",{class:"rounded-lg bg-amber-50/60 border border-amber-200 px-4 py-2 mt-2 text-xs text-slate-700"},[t("div",{class:"text-amber-600 font-bold text-sm mb-1"},"一看就懂：强化学习如何「训练」？"),t("div",{class:"flex items-center gap-4"},[t("div",null,[t("div",{class:"text-xs text-slate-400"},"训练小狗"),t("div",null,[s("坐下 → 给零食 "),t("span",{class:"text-emerald-600 font-bold"},"(+1)"),s(" → 下次更愿意坐下")]),t("div",null,[s("咬沙发 → 被训斥 "),t("span",{class:"text-rose-600 font-bold"},"(-1)"),s(" → 下次减少咬沙发")])]),t("span",{class:"text-amber-400 text-lg font-bold"},"≈"),t("div",null,[t("div",{class:"text-xs text-slate-400"},"训练 AI 下棋"),t("div",null,[s("赢棋 → 奖励信号 "),t("span",{class:"text-emerald-600 font-bold"},"(+1)"),s(" → 强化获胜策略")]),t("div",null,[s("输棋 → 惩罚信号 "),t("span",{class:"text-rose-600 font-bold"},"(-1)"),s(" → 避免失败走法")])])])],-1),t("div",{class:"mt-3 grid grid-cols-2 gap-5 max-w-4xl mx-auto text-sm"},[t("div",{class:"space-y-1.5"},[t("div",{class:"text-amber-600 font-bold"},"关键里程碑"),t("div",{class:"text-slate-600 text-xs space-y-1"},[t("div",null,[t("span",{class:"text-indigo-600 font-medium"},"1948"),s(),t("span",{class:"font-bold text-amber-600"},"控制论"),s(" — Wiener 提出反馈控制的数学框架")]),t("div",null,[t("span",{class:"text-indigo-600 font-medium"},"1950s"),s(),t("span",{class:"font-bold text-amber-600"},"动态规划"),s(" — Bellman 奠定最优序贯决策的理论基础")]),t("div",null,[t("span",{class:"text-indigo-600 font-medium"},"1992"),s(),t("span",{class:"font-bold text-amber-600"},"TD-Gammon"),s(" — 通过自对弈学会双陆棋，达到世界级水平")]),t("div",null,[t("span",{class:"text-indigo-600 font-medium"},"2013"),s(),t("span",{class:"font-bold text-amber-600"},"DQN"),s(" — DeepMind 用深度 Q 网络玩 Atari 超越人类，深度 RL 诞生")]),t("div",null,[t("span",{class:"text-indigo-600 font-medium"},"2016"),s(),t("span",{class:"font-bold text-amber-600"},"AlphaGo"),s(" — 击败李世石；2017 AlphaGo Zero 从零自对弈超越所有前代")])])]),t("div",{class:"space-y-1.5"},[t("div",{class:"text-amber-600 font-bold"},"优点与局限"),t("div",{class:"text-slate-600 text-xs space-y-1"},[t("div",null,[t("span",{class:"text-emerald-500 font-medium"},"✓"),s(),t("span",{class:"font-bold"},"无需标注数据"),s("，从交互经验中学习")]),t("div",null,[t("span",{class:"text-emerald-500 font-medium"},"✓"),s(" 天然适合"),t("span",{class:"font-bold"},"序贯决策"),s("（游戏、机器人控制、对话）")]),t("div",null,[t("span",{class:"text-emerald-500 font-medium"},"✓"),s(" 可与连接主义结合——"),t("span",{class:"font-bold"},"深度 RL"),s(" 是当前最强决策框架")]),t("div",null,[t("span",{class:"text-amber-600 font-medium"},"✗"),s(),t("span",{class:"font-bold text-amber-700"},"奖励设计难"),s("：稍有不慎就会 reward hacking")]),t("div",null,[t("span",{class:"text-amber-600 font-medium"},"✗"),s(),t("span",{class:"font-bold text-amber-700"},"样本效率低"),s("：AlphaGo 自对弈数百万局才达人类水平")]),t("div",null,[t("span",{class:"text-amber-600 font-medium"},"✗"),s(),t("span",{class:"font-bold text-amber-700"},"安全性"),s("与可预测性仍是开放挑战")])])])],-1),t("div",{class:"mt-2 grid grid-cols-5 gap-3 max-w-4xl mx-auto"},[t("div",{class:"col-span-2 rounded-lg bg-amber-100/60 border-l-4 border-amber-500 pl-3 py-1.5 text-xs text-slate-600"},[t("span",{class:"font-bold text-amber-700"},"核心洞察："),s("RLVR（可验证奖励的强化学习）正是行为主义与连接主义的最新融合——今天的大模型对齐离不开它 ")]),t("div",{class:"col-span-3 rounded-lg bg-amber-50/80 border-l-4 border-amber-300 pl-3 py-1.5 text-xs text-slate-600"},[t("span",{class:"font-bold text-amber-600"},"案例：AlphaGo Zero（2017）"),s(" — 不使用任何人类棋谱，仅靠自对弈 + 奖励信号，3 天超越所有人类棋手。"),t("span",{class:"font-medium text-amber-700"},"证明纯粹的「试错 + 奖励」就能涌现超人智能。")])],-1)])]),_:1},16))}};export{h as default};
